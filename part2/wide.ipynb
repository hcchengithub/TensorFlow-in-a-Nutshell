{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peforth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"Name\", \"Sex\", \"Embarked\", \"Cabin\"]\n",
    "CONTINUOUS_COLUMNS = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"PassengerId\", \"Pclass\"]\n",
    "SURVIVED_COLUMN = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = tf.feature_column.categorical_column_with_vocabulary_list(\"Sex\",[\"female\", \"male\"])\n",
    "embarked = tf.feature_column.categorical_column_with_vocabulary_list(\"Embarked\",[\"C\",\"S\",\"Q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "  cabin = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      \"Cabin\", hash_bucket_size=1000)\n",
    "  name = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      \"Name\", hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Continuous columns\n",
    "  age = tf.feature_column.numeric_column(\"Age\")\n",
    "  passenger_id = tf.feature_column.numeric_column(\"PassengerId\")\n",
    "  sib_sp = tf.feature_column.numeric_column(\"SibSp\")\n",
    "  parch = tf.feature_column.numeric_column(\"Parch\")\n",
    "  fare = tf.feature_column.numeric_column(\"Fare\")\n",
    "  p_class = tf.feature_column.numeric_column(\"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Transformations.\n",
    "  age_buckets = tf.feature_column.bucketized_column(age,\n",
    "                                                    boundaries=[\n",
    "                                                        5, 18, 25, 30, 35, 40,\n",
    "                                                        45, 50, 55, 65\n",
    "                                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Wide columns and deep columns.\n",
    "  wide_columns = [sex, embarked, cabin, name, age_buckets,\n",
    "                  tf.feature_column.crossed_column(\n",
    "                      [age_buckets, sex],\n",
    "                      hash_bucket_size=int(1e6)),\n",
    "                  tf.feature_column.crossed_column([embarked, 'name'],\n",
    "                                                   hash_bucket_size=int(1e4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='Age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(5, 18, 25, 30, 35, 40, 45, 50, 55, 65)), _VocabularyListCategoricalColumn(key='Sex', vocabulary_list=('female', 'male'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=1000000, hash_key=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.feature_column.crossed_column([age_buckets, sex],hash_bucket_size=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CrossedColumn(keys=(_VocabularyListCategoricalColumn(key='Embarked', vocabulary_list=('C', 'S', 'Q'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'name'), hash_bucket_size=10000, hash_key=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.feature_column.crossed_column([embarked, 'name'],hash_bucket_size=int(1e4))\n",
    "# 這行出錯 ValueError: categorical_column_with_hash_bucket is not supported for crossing. \n",
    "# Hashing before crossing will increase probability of collision. Instead, use the feature name as a string.\n",
    "# [ ] 從 2018 新 demo 裡去找答案範例 . . . 看到 GitHub\\models\\official\\wide_deep\\census_dataset.py 的例子，\n",
    "#     改用 'Name' 或 'name' 都會好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "  deep_columns = [\n",
    "      tf.feature_column.embedding_column(sex, dimension=8),\n",
    "      tf.feature_column.embedding_column(embarked, dimension=8),\n",
    "      tf.feature_column.embedding_column(cabin, dimension=8),\n",
    "      tf.feature_column.embedding_column(name, dimension=8),\n",
    "      age,\n",
    "      passenger_id,\n",
    "      sib_sp,\n",
    "      parch,\n",
    "      fare,\n",
    "      p_class\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hcche\\AppData\\Local\\Temp\\tmp206bhb4g\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\hcche\\\\AppData\\\\Local\\\\Temp\\\\tmp206bhb4g', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016BD8AD1940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier at 0x16bd8ad1390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  #return \n",
    "tf.estimator.DNNLinearCombinedClassifier(\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以上已經解決了 def build_estimator(model_dir) 的所有問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model directory = ./models\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hcche\\AppData\\Local\\Temp\\tmpbiik21pi\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\hcche\\\\AppData\\\\Local\\\\Temp\\\\tmpbiik21pi', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016BD8BC78D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "<generator object Estimator.predict at 0x0000016BD8BBF728>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find trained model in model_dir: C:\\Users\\hcche\\AppData\\Local\\Temp\\tmpbiik21pi.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-58e49954198e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-58e49954198e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m   \u001b[0mtrain_and_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-58e49954198e>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;31m# m.fit(input_fn=lambda: input_fn(df_train, True), steps=200)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m   \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m   \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_evaluate_model\u001b[1;34m(self, input_fn, hooks, checkpoint_path, name)\u001b[0m\n\u001b[0;32m   1071\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlatest_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         raise ValueError('Could not find trained model in model_dir: {}.'.\n\u001b[1;32m-> 1073\u001b[1;33m                          format(self._model_dir))\n\u001b[0m\u001b[0;32m   1074\u001b[0m       \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatest_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not find trained model in model_dir: C:\\Users\\hcche\\AppData\\Local\\Temp\\tmpbiik21pi."
     ]
    }
   ],
   "source": [
    "def build_estimator(model_dir):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  # Categorical columns\n",
    "  sex = tf.feature_column.categorical_column_with_vocabulary_list(\"Sex\",[\"female\", \"male\"])\n",
    "  embarked = tf.feature_column.categorical_column_with_vocabulary_list(\"Embarked\",[\"C\",\"S\",\"Q\"])\n",
    "\n",
    "  cabin = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      \"Cabin\", hash_bucket_size=1000)\n",
    "  name = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      \"Name\", hash_bucket_size=1000)\n",
    "\n",
    "  # Continuous columns\n",
    "  age = tf.feature_column.numeric_column(\"Age\")\n",
    "  passenger_id = tf.feature_column.numeric_column(\"PassengerId\")\n",
    "  sib_sp = tf.feature_column.numeric_column(\"SibSp\")\n",
    "  parch = tf.feature_column.numeric_column(\"Parch\")\n",
    "  fare = tf.feature_column.numeric_column(\"Fare\")\n",
    "  p_class = tf.feature_column.numeric_column(\"Pclass\")\n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.feature_column.bucketized_column(age,\n",
    "                                                    boundaries=[\n",
    "                                                        5, 18, 25, 30, 35, 40,\n",
    "                                                        45, 50, 55, 65\n",
    "                                                    ])\n",
    "# Wide columns and deep columns.\n",
    "  wide_columns = [sex, embarked, cabin, name, age_buckets,\n",
    "                  tf.feature_column.crossed_column(\n",
    "                      [age_buckets, sex],\n",
    "                      hash_bucket_size=int(1e6)),\n",
    "                  tf.feature_column.crossed_column([embarked, 'name'],\n",
    "                                                   hash_bucket_size=int(1e4))]    \n",
    "  deep_columns = [\n",
    "      tf.feature_column.embedding_column(sex, dimension=8),\n",
    "      tf.feature_column.embedding_column(embarked, dimension=8),\n",
    "      tf.feature_column.embedding_column(cabin, dimension=8),\n",
    "      tf.feature_column.embedding_column(name, dimension=8),\n",
    "      age,\n",
    "      passenger_id,\n",
    "      sib_sp,\n",
    "      parch,\n",
    "      fare,\n",
    "      p_class\n",
    "  ]\n",
    "\n",
    "  return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])\n",
    "\n",
    "def input_fn(df, train=False):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "    indices=[[i, 0] for i in range(df[k].size)],\n",
    "    values=df[k].values,\n",
    "    shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  if train:\n",
    "    label = tf.constant(df[SURVIVED_COLUMN].values)\n",
    "      # Returns the feature columns and the label.\n",
    "    return feature_cols, label\n",
    "  else:\n",
    "    return feature_cols\n",
    "\n",
    "\n",
    "def train_and_eval():\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(\"./train.csv\"),\n",
    "      skipinitialspace=True)\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(\"./test.csv\"),\n",
    "      skipinitialspace=True)\n",
    "\n",
    "  model_dir = \"./models\"\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir)\n",
    "  # m.fit(input_fn=lambda: input_fn(df_train, True), steps=200)\n",
    "  print (m.predict(input_fn=lambda: input_fn(df_test)))\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_train, True), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "\n",
    "def main(_):\n",
    "  train_and_eval()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reDef unknown\n"
     ]
    }
   ],
   "source": [
    "%%f if playing in jupyter notebook \n",
    "\\ Now we redefine the 'unknown' command that does nothing at default  \n",
    ": unknown ( token -- thing Y|N) // Try to find the unknown token in __main__\n",
    "  py> getattr(sys.modules['__main__'],pop(),\"Ûnknôwn\") \n",
    "  py> str(tos())==\"Ûnknôwn\" if drop false else true then ;\n",
    "  /// here after, when FORTH come accross an unknown token, instead of alerting \n",
    "  /// it try to find the token in python __main__ module name space.\n",
    ": add-path ( <path> -- ) // Add path to sys.path so \"import module-name\" can find the module\n",
    "    CR word trim ( \"path\" ) py: sys.path.append(pop()) ;\n",
    "code # print(nexttoken('\\n')) end-code // print the # leading comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__name__ tib.\n",
      "__name__ tib. \\ ==> __main__ (<class 'str'>)\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "%f *debug*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
